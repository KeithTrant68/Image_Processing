{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D5H74exrlMMa"
      },
      "source": [
        "# Thực hành: Phát hiện đối tượng YOLOv3  \n",
        "Trong bài này ta sẽ thực hành bài toán phát hiện quả dừa trên băng chuyền bằng mô hình YOLOv3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KeqnY5KjpBUb"
      },
      "source": [
        "# 1. Cài đặt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uEXu7qOHlMMc"
      },
      "source": [
        "Tải YOLOv3"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Git clone https://github.com/ultralytics/yolov3"
      ],
      "metadata": {
        "id": "PZstFSMfvJtE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hTvV2DIwpi0T",
        "outputId": "a19acafd-8c20-426f-96c8-8690eae2bcd7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'yolov3'...\n",
            "remote: Enumerating objects: 10034, done.\u001b[K\n",
            "remote: Counting objects: 100% (17/17), done.\u001b[K\n",
            "remote: Compressing objects: 100% (14/14), done.\u001b[K\n",
            "remote: Total 10034 (delta 4), reused 9 (delta 3), pack-reused 10017\u001b[K\n",
            "Receiving objects: 100% (10034/10034), 9.36 MiB | 14.32 MiB/s, done.\n",
            "Resolving deltas: 100% (6762/6762), done.\n"
          ]
        }
      ],
      "source": [
        "#############\n",
        "# 1 line of code here\n",
        "#############"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lc-fub82pvdM",
        "outputId": "56b3705d-616f-40bd-b7f7-06ebbc406276"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/yolov3\n"
          ]
        }
      ],
      "source": [
        "cd yolov3/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NxOzD2kilMMe"
      },
      "source": [
        "Cài đặt các thư viện cần thiết sử dụng pip install requrement.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "phIdfLIjp1qs",
        "outputId": "85f968c1-2503-4e7f-fd01-0d9850fd0b65"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: matplotlib>=3.2.2 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 4)) (3.2.2)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 5)) (1.21.6)\n",
            "Requirement already satisfied: opencv-python>=4.1.2 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 6)) (4.6.0.66)\n",
            "Requirement already satisfied: Pillow>=7.1.2 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 7)) (7.1.2)\n",
            "Requirement already satisfied: PyYAML>=5.3.1 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 8)) (6.0)\n",
            "Requirement already satisfied: requests>=2.23.0 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 9)) (2.23.0)\n",
            "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 10)) (1.7.3)\n",
            "Requirement already satisfied: torch>=1.7.0 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 11)) (1.12.1+cu113)\n",
            "Requirement already satisfied: torchvision>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 12)) (0.13.1+cu113)\n",
            "Requirement already satisfied: tqdm>=4.41.0 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 13)) (4.64.1)\n",
            "Requirement already satisfied: tensorboard>=2.4.1 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 16)) (2.8.0)\n",
            "Collecting wandb\n",
            "  Downloading wandb-0.13.3-py2.py3-none-any.whl (1.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.8 MB 5.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pandas>=1.1.4 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 20)) (1.3.5)\n",
            "Requirement already satisfied: seaborn>=0.11.0 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 21)) (0.11.2)\n",
            "Collecting thop\n",
            "  Downloading thop-0.1.1.post2209072238-py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.2.2->-r requirements.txt (line 4)) (2.8.2)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.2.2->-r requirements.txt (line 4)) (3.0.9)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.2.2->-r requirements.txt (line 4)) (1.4.4)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3.2.2->-r requirements.txt (line 4)) (0.11.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.23.0->-r requirements.txt (line 9)) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.23.0->-r requirements.txt (line 9)) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.23.0->-r requirements.txt (line 9)) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.23.0->-r requirements.txt (line 9)) (2022.6.15)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.7.0->-r requirements.txt (line 11)) (4.1.1)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.4.1->-r requirements.txt (line 16)) (0.6.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.4.1->-r requirements.txt (line 16)) (1.0.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.4.1->-r requirements.txt (line 16)) (1.35.0)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.4.1->-r requirements.txt (line 16)) (57.4.0)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.4.1->-r requirements.txt (line 16)) (1.2.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.4.1->-r requirements.txt (line 16)) (0.37.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.4.1->-r requirements.txt (line 16)) (3.4.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.4.1->-r requirements.txt (line 16)) (1.8.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.4.1->-r requirements.txt (line 16)) (0.4.6)\n",
            "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.4.1->-r requirements.txt (line 16)) (1.48.1)\n",
            "Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.4.1->-r requirements.txt (line 16)) (3.17.3)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=1.1.4->-r requirements.txt (line 20)) (2022.2.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.4.1->-r requirements.txt (line 16)) (4.9)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.4.1->-r requirements.txt (line 16)) (0.2.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.4.1->-r requirements.txt (line 16)) (4.2.4)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.4.1->-r requirements.txt (line 16)) (1.15.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.4.1->-r requirements.txt (line 16)) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard>=2.4.1->-r requirements.txt (line 16)) (4.12.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard>=2.4.1->-r requirements.txt (line 16)) (3.8.1)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard>=2.4.1->-r requirements.txt (line 16)) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.4.1->-r requirements.txt (line 16)) (3.2.0)\n",
            "Requirement already satisfied: Click!=8.0.0,>=7.0 in /usr/local/lib/python3.7/dist-packages (from wandb->-r requirements.txt (line 17)) (7.1.2)\n",
            "Collecting setproctitle\n",
            "  Downloading setproctitle-1.3.2-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n",
            "Collecting sentry-sdk>=1.0.0\n",
            "  Downloading sentry_sdk-1.9.8-py2.py3-none-any.whl (158 kB)\n",
            "\u001b[K     |████████████████████████████████| 158 kB 69.1 MB/s \n",
            "\u001b[?25hCollecting docker-pycreds>=0.4.0\n",
            "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
            "Collecting shortuuid>=0.5.0\n",
            "  Downloading shortuuid-1.0.9-py3-none-any.whl (9.4 kB)\n",
            "Requirement already satisfied: promise<3,>=2.0 in /usr/local/lib/python3.7/dist-packages (from wandb->-r requirements.txt (line 17)) (2.3)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb->-r requirements.txt (line 17)) (5.4.8)\n",
            "Collecting pathtools\n",
            "  Downloading pathtools-0.1.2.tar.gz (11 kB)\n",
            "Collecting GitPython>=1.0.0\n",
            "  Downloading GitPython-3.1.27-py3-none-any.whl (181 kB)\n",
            "\u001b[K     |████████████████████████████████| 181 kB 63.5 MB/s \n",
            "\u001b[?25hCollecting gitdb<5,>=4.0.1\n",
            "  Downloading gitdb-4.0.9-py3-none-any.whl (63 kB)\n",
            "\u001b[K     |████████████████████████████████| 63 kB 2.3 MB/s \n",
            "\u001b[?25hCollecting smmap<6,>=3.0.1\n",
            "  Downloading smmap-5.0.0-py3-none-any.whl (24 kB)\n",
            "Collecting sentry-sdk>=1.0.0\n",
            "  Downloading sentry_sdk-1.9.7-py2.py3-none-any.whl (157 kB)\n",
            "\u001b[K     |████████████████████████████████| 157 kB 74.4 MB/s \n",
            "\u001b[?25h  Downloading sentry_sdk-1.9.6-py2.py3-none-any.whl (157 kB)\n",
            "\u001b[K     |████████████████████████████████| 157 kB 79.7 MB/s \n",
            "\u001b[?25h  Downloading sentry_sdk-1.9.5-py2.py3-none-any.whl (157 kB)\n",
            "\u001b[K     |████████████████████████████████| 157 kB 75.3 MB/s \n",
            "\u001b[?25h  Downloading sentry_sdk-1.9.4-py2.py3-none-any.whl (157 kB)\n",
            "\u001b[K     |████████████████████████████████| 157 kB 77.5 MB/s \n",
            "\u001b[?25h  Downloading sentry_sdk-1.9.3-py2.py3-none-any.whl (157 kB)\n",
            "\u001b[K     |████████████████████████████████| 157 kB 75.9 MB/s \n",
            "\u001b[?25h  Downloading sentry_sdk-1.9.2-py2.py3-none-any.whl (157 kB)\n",
            "\u001b[K     |████████████████████████████████| 157 kB 81.0 MB/s \n",
            "\u001b[?25h  Downloading sentry_sdk-1.9.1-py2.py3-none-any.whl (157 kB)\n",
            "\u001b[K     |████████████████████████████████| 157 kB 78.7 MB/s \n",
            "\u001b[?25h  Downloading sentry_sdk-1.9.0-py2.py3-none-any.whl (156 kB)\n",
            "\u001b[K     |████████████████████████████████| 156 kB 76.3 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pathtools\n",
            "  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pathtools: filename=pathtools-0.1.2-py3-none-any.whl size=8806 sha256=f97a5cba644885793468c2ba1cc15348c5ceb3105254e8d07e4dd23647e86f06\n",
            "  Stored in directory: /root/.cache/pip/wheels/3e/31/09/fa59cef12cdcfecc627b3d24273699f390e71828921b2cbba2\n",
            "Successfully built pathtools\n",
            "Installing collected packages: smmap, gitdb, shortuuid, setproctitle, sentry-sdk, pathtools, GitPython, docker-pycreds, wandb, thop\n",
            "Successfully installed GitPython-3.1.27 docker-pycreds-0.4.0 gitdb-4.0.9 pathtools-0.1.2 sentry-sdk-1.9.0 setproctitle-1.3.2 shortuuid-1.0.9 smmap-5.0.0 thop-0.1.1.post2209072238 wandb-0.13.3\n"
          ]
        }
      ],
      "source": [
        "#############\n",
        "# 1 line of code here\n",
        "#############"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kOdE5IWylMMf"
      },
      "source": [
        "# 2. Gán nhãn dữ liệu  \n",
        "Làm quen với gán nhãn dữ liệu bằng công cụ LabelImg\n",
        "1. Cài đặt công cụ LabelImg: https://github.com/tzutalin/labelImg  \n",
        "2. Chỉnh sửa predefined_classes.txt trong thư mục labelimg/data: Xoá nội dung 15 class có sẵn và thay bằng coconut  \n",
        "3. Mở labelimg: python3 labelimg  \n",
        "4. Chọn thư mục samples, chọn định dạng YOLO và thực hiện đánh nhãn \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BA75f7ImqNVK"
      },
      "source": [
        "# 3. Chuẩn bị dữ liệu\n",
        "\n",
        "**Mô tả**: Trong thư mục datasets chứa bộ dữ liệu sử dụng trong bài thực hành này. Trong thư mục này chứa thư mục con **images** gồm toàn bộ ảnh dữ liệu; file **all_annotations.txt** chứa nhãn bounding box của toàn bộ ảnh (được đánh nhãn sẵn từ trước). File **all_annotations.txt** gồm nhiều dòng, mỗi dòng có format như sau:\n",
        "\n",
        "_image_path x1,y1,u1,v1,c1 x2,y2,u2,v2,c2 ... xn, yn, un, vn, cn_\n",
        "\n",
        "Trong đó (xi, yi) là tọa độ góc trên trái của đối tượng thứ i, (ui, vi) là tọa độ góc trên phải của đối tượng thứ i, ci là lớp của đối tượng thứ i. Trong bài thực hành này ta chỉ có một lớp dữ liệu duy nhất.\n",
        "\n",
        "\n",
        "**Mô tả format annotation mới**: Mỗi ảnh ta sẽ có 1 file .txt lưu thông tin các bounding box. File .txt này có định dạng như sau:\n",
        "\n",
        "*   Mỗi bounding box một dòng trong file\n",
        "*   Format của từng dòng là: class x_center y_center width height\n",
        "*   Cần normalize x_center y_center width height về range [0, 1]\n",
        "*   class được đánh số bắt đầu từ 0\n",
        "\n",
        "**Công việc cần thực hiện**:  \n",
        "\n",
        "1.   Tạo thư mục datasets/labels chứa toàn bộ các file .txt (mỗi ảnh trong thư mục datasets/images ứng với 1 file .txt trong thư mục datasets/labels) theo mô tả ở trên)\n",
        "2.   Tham khảo file yolov3/data/coco.yaml, tạo file yolov3/data/coconut.yaml ứng với dataset ta vừa xử lý\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Mount Google Drive"
      ],
      "metadata": {
        "id": "QQVrJdEQvlD0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#############\n",
        "# your code here\n",
        "#############"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rkdgbs40yXGt",
        "outputId": "a3cbc31b-1c7a-4dd0-dc11-f7fb825b6fd1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0pPilJXQsbhS"
      },
      "source": [
        "Upload datasets.zip và unzip\n",
        "\n",
        "**Lưu ý:** Trong file zip sẵn chỉ có 20 ảnh. Nếu muốn tải full dữ liệu, các bạn có thể tải tại link sau: <a href=\"https://u.pcloud.link/publink/show?code=XZp1BhVZmloQdDfGGHFnNfoK7gX2vy66NlrX\">https://u.pcloud.link/publink/show?code=XZp1BhVZmloQdDfGGHFnNfoK7gX2vy66NlrX</a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6BowDZGxs1Nh",
        "outputId": "e73d2a78-887a-42d6-d1d9-fe2c77457360"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "datasets.zip  drive  sample_data  yolov3\n"
          ]
        }
      ],
      "source": [
        "!ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EO3t-nYnnvHo",
        "outputId": "397254d2-3920-4f0d-82ff-31af6b771c32"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ],
      "source": [
        "cd /content/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YmkNIBIDtKWr",
        "outputId": "8f33fcfa-a690-4a46-8df1-d715121adcbd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  datasets.zip\n",
            "   creating: datasets/\n",
            "  inflating: __MACOSX/._datasets     \n",
            "  inflating: datasets/.DS_Store      \n",
            "  inflating: __MACOSX/datasets/._.DS_Store  \n",
            "   creating: datasets/images/\n",
            "  inflating: __MACOSX/datasets/._images  \n",
            "   creating: datasets/labels/\n",
            "  inflating: __MACOSX/datasets/._labels  \n",
            "  inflating: datasets/all_annotations.txt  \n",
            "  inflating: __MACOSX/datasets/._all_annotations.txt  \n",
            "  inflating: datasets/images/.DS_Store  \n",
            "  inflating: __MACOSX/datasets/images/._.DS_Store  \n",
            "  inflating: datasets/images/8.png   \n",
            "  inflating: __MACOSX/datasets/images/._8.png  \n",
            "  inflating: datasets/images/9.png   \n",
            "  inflating: __MACOSX/datasets/images/._9.png  \n",
            "  inflating: datasets/images/14.png  \n",
            "  inflating: __MACOSX/datasets/images/._14.png  \n",
            "  inflating: datasets/images/15.png  \n",
            "  inflating: __MACOSX/datasets/images/._15.png  \n",
            "  inflating: datasets/images/17.png  \n",
            "  inflating: __MACOSX/datasets/images/._17.png  \n",
            "  inflating: datasets/images/16.png  \n",
            "  inflating: __MACOSX/datasets/images/._16.png  \n",
            "  inflating: datasets/images/12.png  \n",
            "  inflating: __MACOSX/datasets/images/._12.png  \n",
            "  inflating: datasets/images/13.png  \n",
            "  inflating: __MACOSX/datasets/images/._13.png  \n",
            "  inflating: datasets/images/11.png  \n",
            "  inflating: __MACOSX/datasets/images/._11.png  \n",
            "  inflating: datasets/images/10.png  \n",
            "  inflating: __MACOSX/datasets/images/._10.png  \n",
            "  inflating: datasets/images/20.png  \n",
            "  inflating: __MACOSX/datasets/images/._20.png  \n",
            "  inflating: datasets/images/18.png  \n",
            "  inflating: __MACOSX/datasets/images/._18.png  \n",
            "  inflating: datasets/images/19.png  \n",
            "  inflating: __MACOSX/datasets/images/._19.png  \n",
            "  inflating: datasets/images/4.png   \n",
            "  inflating: __MACOSX/datasets/images/._4.png  \n",
            "  inflating: datasets/images/5.png   \n",
            "  inflating: __MACOSX/datasets/images/._5.png  \n",
            "  inflating: datasets/images/7.png   \n",
            "  inflating: __MACOSX/datasets/images/._7.png  \n",
            "  inflating: datasets/images/6.png   \n",
            "  inflating: __MACOSX/datasets/images/._6.png  \n",
            "  inflating: datasets/images/2.png   \n",
            "  inflating: __MACOSX/datasets/images/._2.png  \n",
            "  inflating: datasets/images/3.png   \n",
            "  inflating: __MACOSX/datasets/images/._3.png  \n",
            "  inflating: datasets/images/1.png   \n",
            "  inflating: __MACOSX/datasets/images/._1.png  \n",
            "  inflating: datasets/labels/.DS_Store  \n",
            "  inflating: __MACOSX/datasets/labels/._.DS_Store  \n"
          ]
        }
      ],
      "source": [
        "!unzip datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WcfTEKGOqaf6"
      },
      "outputs": [],
      "source": [
        "# Bước 1: Sinh file .txt chứa thông tin bounding box cho từng ảnh\n",
        "import os\n",
        "import cv2\n",
        "\n",
        "IMAGE_FOLDER = 'datasets/images'\n",
        "ANNOTATION_FILE = 'datasets/all_annotations.txt'\n",
        "LABEL_FOLDER = 'datasets/labels'\n",
        "\n",
        "if not os.path.isdir(LABEL_FOLDER):\n",
        "  os.makedir(LABEL_FOLDER)\n",
        "\n",
        "with open(ANNOTATION_FILE) as f:\n",
        "  for line in f:\n",
        "    data = line.split()\n",
        "\n",
        "    image_fp = data[0]\n",
        "    image_idx = image_fp.split('/')[-1].split('.')[0]\n",
        "    image = cv2.imread(image_fp)\n",
        "    if image is None:\n",
        "      continue\n",
        "    height, width = image.shape[:2]\n",
        "\n",
        "    normalized_bbox = []\n",
        "    for bbox in data[1:]:\n",
        "      x, y, u, v, cls = [int(a) for a in bbox.split(',')]\n",
        "      x_center = (x + u) / 2 / width\n",
        "      y_center = (y + v) / 2 / height\n",
        "      box_width = (u - x) / width\n",
        "      box_height =  (v - y) / height\n",
        "\n",
        "      normalized_bbox.append((cls, x_center, y_center, box_width, box_height))\n",
        "\n",
        "    with open(os.path.join(LABEL_FOLDER, image_idx + '.txt'), 'w') as g:\n",
        "      for bbox in normalized_bbox:\n",
        "        g.write(' '.join([str(s) for s in bbox]) + '\\n')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cd yolov3/data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k0vHkVIpzZ5p",
        "outputId": "ea68528a-e256-4e8e-8080-810c8138fb24"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/yolov3/data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cp coco.yaml coconut.yaml"
      ],
      "metadata": {
        "id": "--nU0OWozj21"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 130
        },
        "id": "lqRWiJHmOocg",
        "outputId": "0de827e6-5acb-400e-801c-be93f6e08868"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-14-083b56b11b6b>\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    cd yolov3/data\u001b[0m\n\u001b[0m            ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ],
      "source": [
        "# Bước 2: Tạo file coconut.yaml\n",
        "cd yolov3/data\n",
        "cp coco.yaml coconut.yaml\n",
        "\n",
        "# Chỉnh sửa yolov3/data/coconut.yaml với nội dung như trong phần comment sau:\n",
        "\n",
        "'''\n",
        "path: ../datasets\n",
        "train: images/\n",
        "val: images/  \n",
        "\n",
        "nc: 1\n",
        "\n",
        "names: ['coconut']\n",
        "'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cv4mHcBrPKJB"
      },
      "source": [
        "# 4. Huấn luyện mô hình"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qUBQcmqF19h3",
        "outputId": "39620429-3cfc-4fe9-97b7-61654f4ab482"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/yolov3\n"
          ]
        }
      ],
      "source": [
        "cd /content/yolov3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "al5OTHQDPhf3"
      },
      "source": [
        "## Cách 1: Finetune\n",
        "\n",
        "Ta sẽ thử fine-tune model với pretrained có sẵn, các tham số theo như mặc định (tham khảo file yolov3/train.py), đây là cách chúng ta luôn thử đầu tiên với một bài toán mới. Bạn có thể thay đổi số epoch để rút ngắn thời gian training. Sau khi kết thúc quá trình training, hãy nhìn vào phần log cuối cùng để xác định vị trí lưu weights đã train."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ok3gZu7W0f2g",
        "outputId": "2d6f8a9d-838e-4965-8a5e-b9410868cb01"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://ultralytics.com/assets/Arial.ttf to /root/.config/Ultralytics/Arial.ttf...\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: (1) Create a W&B account\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: (2) Use an existing W&B account\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: (3) Don't visualize my results\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Enter your choice: (30 second timeout) \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: W&B disabled due to login timeout.\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mweights=yolov3.pt, cfg=, data=data/coconut.yaml, hyp=data/hyps/hyp.scratch.yaml, epochs=20, batch_size=16, imgsz=416, rect=False, resume=False, nosave=False, noval=False, noautoanchor=False, evolve=None, bucket=, cache=None, image_weights=False, device=, multi_scale=False, single_cls=False, adam=False, sync_bn=False, workers=8, project=runs/train, name=exp, exist_ok=False, quad=False, linear_lr=False, label_smoothing=0.0, patience=100, freeze=0, save_period=-1, local_rank=-1, entity=None, upload_dataset=False, bbox_interval=-1, artifact_alias=latest\n",
            "\u001b[34m\u001b[1mgithub: \u001b[0mup to date with https://github.com/ultralytics/yolov3 ✅\n",
            "YOLOv3 🚀 v9.6.0-22-g0bbd055 torch 1.12.1+cu113 CUDA:0 (Tesla T4, 15110MiB)\n",
            "\n",
            "\u001b[34m\u001b[1mhyperparameters: \u001b[0mlr0=0.01, lrf=0.1, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=0.05, cls=0.5, cls_pw=1.0, obj=1.0, obj_pw=1.0, iou_t=0.2, anchor_t=4.0, fl_gamma=0.0, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0\n",
            "\u001b[34m\u001b[1mWeights & Biases: \u001b[0mrun 'pip install wandb' to automatically track and visualize YOLOv3 🚀 runs (RECOMMENDED)\n",
            "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs/train', view at http://localhost:6006/\n",
            "Downloading https://github.com/ultralytics/yolov3/releases/download/v9.6.0/yolov3.pt to yolov3.pt...\n",
            "100% 119M/119M [00:03<00:00, 38.7MB/s]\n",
            "\n",
            "Overriding model.yaml nc=80 with nc=1\n",
            "\n",
            "                 from  n    params  module                                  arguments                     \n",
            "  0                -1  1       928  models.common.Conv                      [3, 32, 3, 1]                 \n",
            "  1                -1  1     18560  models.common.Conv                      [32, 64, 3, 2]                \n",
            "  2                -1  1     20672  models.common.Bottleneck                [64, 64]                      \n",
            "  3                -1  1     73984  models.common.Conv                      [64, 128, 3, 2]               \n",
            "  4                -1  2    164608  models.common.Bottleneck                [128, 128]                    \n",
            "  5                -1  1    295424  models.common.Conv                      [128, 256, 3, 2]              \n",
            "  6                -1  8   2627584  models.common.Bottleneck                [256, 256]                    \n",
            "  7                -1  1   1180672  models.common.Conv                      [256, 512, 3, 2]              \n",
            "  8                -1  8  10498048  models.common.Bottleneck                [512, 512]                    \n",
            "  9                -1  1   4720640  models.common.Conv                      [512, 1024, 3, 2]             \n",
            " 10                -1  4  20983808  models.common.Bottleneck                [1024, 1024]                  \n",
            " 11                -1  1   5245952  models.common.Bottleneck                [1024, 1024, False]           \n",
            " 12                -1  1    525312  models.common.Conv                      [1024, 512, 1, 1]             \n",
            " 13                -1  1   4720640  models.common.Conv                      [512, 1024, 3, 1]             \n",
            " 14                -1  1    525312  models.common.Conv                      [1024, 512, 1, 1]             \n",
            " 15                -1  1   4720640  models.common.Conv                      [512, 1024, 3, 1]             \n",
            " 16                -2  1    131584  models.common.Conv                      [512, 256, 1, 1]              \n",
            " 17                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
            " 18           [-1, 8]  1         0  models.common.Concat                    [1]                           \n",
            " 19                -1  1   1377792  models.common.Bottleneck                [768, 512, False]             \n",
            " 20                -1  1   1312256  models.common.Bottleneck                [512, 512, False]             \n",
            " 21                -1  1    131584  models.common.Conv                      [512, 256, 1, 1]              \n",
            " 22                -1  1   1180672  models.common.Conv                      [256, 512, 3, 1]              \n",
            " 23                -2  1     33024  models.common.Conv                      [256, 128, 1, 1]              \n",
            " 24                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
            " 25           [-1, 6]  1         0  models.common.Concat                    [1]                           \n",
            " 26                -1  1    344832  models.common.Bottleneck                [384, 256, False]             \n",
            " 27                -1  2    656896  models.common.Bottleneck                [256, 256, False]             \n",
            " 28      [27, 22, 15]  1     32310  models.yolo.Detect                      [1, [[10, 13, 16, 30, 33, 23], [30, 61, 62, 45, 59, 119], [116, 90, 156, 198, 373, 326]], [256, 512, 1024]]\n",
            "Model Summary: 333 layers, 61523734 parameters, 61523734 gradients, 155.3 GFLOPs\n",
            "\n",
            "Transferred 433/439 items from yolov3.pt\n",
            "Scaled weight_decay = 0.0005\n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m SGD with parameter groups 72 weight, 75 weight (no decay), 75 bias\n",
            "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(always_apply=False, p=0.01, blur_limit=(3, 7)), MedianBlur(always_apply=False, p=0.01, blur_limit=(3, 7)), ToGray(always_apply=False, p=0.01), CLAHE(always_apply=False, p=0.01, clip_limit=(1, 4.0), tile_grid_size=(8, 8))\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mScanning '../datasets/labels' images and labels...20 found, 0 missing, 0 empty, 0 corrupted: 100% 20/20 [00:00<00:00, 324.80it/s]\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: ../datasets/labels.cache\n",
            "\u001b[34m\u001b[1mval: \u001b[0mScanning '../datasets/labels.cache' images and labels... 20 found, 0 missing, 0 empty, 0 corrupted: 100% 20/20 [00:00<?, ?it/s]\n",
            "Plotting labels to runs/train/exp/labels.jpg... \n",
            "\n",
            "\u001b[34m\u001b[1mAutoAnchor: \u001b[0m6.04 anchors/target, 1.000 Best Possible Recall (BPR). Current anchors are a good fit to dataset ✅\n",
            "Image sizes 416 train, 416 val\n",
            "Using 2 dataloader workers\n",
            "Logging results to \u001b[1mruns/train/exp\u001b[0m\n",
            "Starting training for 20 epochs...\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
            "      0/19     5.55G    0.1223    0.0619         0        61       416: 100% 2/2 [00:09<00:00,  4.91s/it]\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 1/1 [00:01<00:00,  1.44s/it]\n",
            "                 all         20        104     0.0434      0.106     0.0165    0.00305\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
            "      1/19     5.55G    0.1187   0.04432         0        28       416: 100% 2/2 [00:00<00:00,  2.47it/s]\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 1/1 [00:00<00:00,  2.48it/s]\n",
            "                 all         20        104     0.0448     0.0865     0.0163    0.00306\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
            "      2/19     5.55G    0.1194   0.05066         0        34       416: 100% 2/2 [00:00<00:00,  2.58it/s]\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 1/1 [00:00<00:00,  3.16it/s]\n",
            "                 all         20        104     0.0511     0.0962     0.0184    0.00313\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
            "      3/19     5.55G    0.1189   0.04794         0        34       416: 100% 2/2 [00:00<00:00,  2.98it/s]\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 1/1 [00:00<00:00,  2.77it/s]\n",
            "                 all         20        104      0.051      0.115     0.0186    0.00346\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
            "      4/19     5.55G    0.1174   0.05679         0        48       416: 100% 2/2 [00:00<00:00,  3.08it/s]\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 1/1 [00:00<00:00,  2.54it/s]\n",
            "                 all         20        104     0.0535      0.115     0.0226    0.00422\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
            "      5/19     5.55G     0.117    0.0559         0        41       416: 100% 2/2 [00:00<00:00,  2.71it/s]\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 1/1 [00:00<00:00,  3.31it/s]\n",
            "                 all         20        104     0.0565      0.106     0.0231    0.00463\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
            "      6/19     5.55G    0.1155    0.0653         0        42       416: 100% 2/2 [00:00<00:00,  2.69it/s]\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 1/1 [00:00<00:00,  3.08it/s]\n",
            "                 all         20        104       0.06      0.115     0.0262    0.00509\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
            "      7/19     5.55G    0.1123   0.06001         0        34       416: 100% 2/2 [00:00<00:00,  2.72it/s]\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 1/1 [00:00<00:00,  2.75it/s]\n",
            "                 all         20        104     0.0644      0.125     0.0304    0.00557\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
            "      8/19     5.55G    0.1084   0.06294         0        34       416: 100% 2/2 [00:00<00:00,  2.73it/s]\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 1/1 [00:00<00:00,  2.99it/s]\n",
            "                 all         20        104      0.218     0.0673     0.0401    0.00673\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
            "      9/19     5.55G    0.1085   0.06882         0        46       416: 100% 2/2 [00:00<00:00,  2.74it/s]\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 1/1 [00:00<00:00,  3.09it/s]\n",
            "                 all         20        104     0.0817      0.135     0.0419    0.00709\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
            "     10/19     5.55G    0.1085   0.05616         0        37       416: 100% 2/2 [00:00<00:00,  2.76it/s]\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 1/1 [00:00<00:00,  2.78it/s]\n",
            "                 all         20        104      0.102      0.144     0.0524    0.00852\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
            "     11/19     5.55G    0.1067   0.06275         0        33       416: 100% 2/2 [00:00<00:00,  2.98it/s]\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 1/1 [00:00<00:00,  2.87it/s]\n",
            "                 all         20        104      0.098      0.183     0.0548     0.0103\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
            "     12/19     5.55G    0.1077   0.07933         0        48       416: 100% 2/2 [00:00<00:00,  2.79it/s]\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 1/1 [00:00<00:00,  3.62it/s]\n",
            "                 all         20        104      0.099      0.183     0.0586     0.0107\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
            "     13/19     5.55G    0.1034   0.05839         0        22       416: 100% 2/2 [00:00<00:00,  3.29it/s]\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 1/1 [00:00<00:00,  2.88it/s]\n",
            "                 all         20        104      0.227      0.115      0.072     0.0135\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
            "     14/19     5.55G    0.1026   0.07506         0        41       416: 100% 2/2 [00:00<00:00,  2.82it/s]\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 1/1 [00:00<00:00,  2.90it/s]\n",
            "                 all         20        104      0.219      0.125     0.0819     0.0164\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
            "     15/19     5.55G    0.1016   0.07169         0        34       416: 100% 2/2 [00:00<00:00,  2.57it/s]\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 1/1 [00:00<00:00,  2.76it/s]\n",
            "                 all         20        104      0.342      0.115     0.0881     0.0153\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
            "     16/19     5.55G    0.1007   0.06679         0        29       416: 100% 2/2 [00:00<00:00,  2.61it/s]\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 1/1 [00:00<00:00,  2.86it/s]\n",
            "                 all         20        104      0.282      0.125     0.0979     0.0194\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
            "     17/19     5.55G     0.104   0.08023         0        53       416: 100% 2/2 [00:00<00:00,  2.82it/s]\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 1/1 [00:00<00:00,  2.39it/s]\n",
            "                 all         20        104      0.144      0.221      0.107     0.0214\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
            "     18/19     5.55G   0.09873   0.07616         0        39       416: 100% 2/2 [00:00<00:00,  3.49it/s]\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 1/1 [00:00<00:00,  3.56it/s]\n",
            "                 all         20        104      0.218      0.192      0.112     0.0215\n",
            "\n",
            "     Epoch   gpu_mem       box       obj       cls    labels  img_size\n",
            "     19/19     5.55G    0.0979   0.07868         0        38       416: 100% 2/2 [00:00<00:00,  2.66it/s]\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 1/1 [00:00<00:00,  2.86it/s]\n",
            "                 all         20        104      0.243      0.192      0.115     0.0209\n",
            "\n",
            "20 epochs completed in 0.032 hours.\n",
            "Optimizer stripped from runs/train/exp/weights/last.pt, 123.4MB\n",
            "Optimizer stripped from runs/train/exp/weights/best.pt, 123.4MB\n",
            "\n",
            "Validating runs/train/exp/weights/best.pt...\n",
            "Fusing layers... \n",
            "Model Summary: 261 layers, 61497430 parameters, 0 gradients, 154.5 GFLOPs\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 1/1 [00:00<00:00,  1.74it/s]\n",
            "                 all         20        104       0.22      0.192      0.112     0.0216\n",
            "Results saved to \u001b[1mruns/train/exp\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!python train.py --img 416 --batch 16 --epochs 20 --data data/coconut.yaml --weights yolov3.pt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mG8oitE0n2LY"
      },
      "source": [
        "Thử inference trên 1 video sử dụng model đã finetune ở trên, ta sẽ sử dụng IoU threshold cho phần NMS là 0.5, threshold cho object score là 0.6  \n",
        "\n",
        "Tải video tại link sau: ***https://u.pcloud.link/publink/show?code=XZziBhVZgRqC3dfFdWL1kB1zq0py6BbrMJny***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rvmwahle_vg-",
        "outputId": "01e847eb-d273-4d38-dca2-103c3688a350"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mdetect: \u001b[0mweights=['runs/train/exp/weights/best.pt'], source=../datasets/coconut.mp4, imgsz=[416, 416], conf_thres=0.6, iou_thres=0.5, max_det=1000, device=, view_img=False, save_txt=False, save_conf=False, save_crop=False, nosave=False, classes=None, agnostic_nms=False, augment=False, visualize=False, update=False, project=runs/detect, name=exp, exist_ok=False, line_thickness=3, hide_labels=False, hide_conf=False, half=False, dnn=False\n",
            "YOLOv3 🚀 v9.6.0-3-gb870de5 torch 1.10.0+cu111 CUDA:0 (Tesla K80, 11441MiB)\n",
            "\n",
            "Fusing layers... \n",
            "Model Summary: 261 layers, 61497430 parameters, 0 gradients, 154.7 GFLOPs\n",
            "Traceback (most recent call last):\n",
            "  File \"detect.py\", line 244, in <module>\n",
            "    main(opt)\n",
            "  File \"detect.py\", line 239, in main\n",
            "    run(**vars(opt))\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torch/autograd/grad_mode.py\", line 28, in decorate_context\n",
            "    return func(*args, **kwargs)\n",
            "  File \"detect.py\", line 95, in run\n",
            "    dataset = LoadImages(source, img_size=imgsz, stride=stride, auto=pt and not jit)\n",
            "  File \"/content/yolov3/utils/datasets.py\", line 170, in __init__\n",
            "    raise Exception(f'ERROR: {p} does not exist')\n",
            "Exception: ERROR: /content/datasets/coconut.mp4 does not exist\n"
          ]
        }
      ],
      "source": [
        "!python detect.py --weights runs/train/exp/weights/best.pt --source ../datasets/coconut.mp4 --imgsz 416 --iou-thres 0.5 --conf-thres 0.6"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1nRsKhiTofTY"
      },
      "source": [
        "## Cách 2: Tự cấu hình lại các tham số\n",
        "\n",
        "Để làm chủ việc training model, ta có thể tự thay đổi các tham số để phù hợp với từng bộ dữ liệu khác nhau. Bài toán coconut detection khá đơn giản, vì vậy cách sử dụng pretrained ở trên đã cho kết quả tốt, tuy nhiên với nhiều bài toán phức tạp hơn, ta sẽ cần thay đổi các tham số sao cho phù hợp. Ở đây ta sẽ cấu hình 2 phần chính: \n",
        "\n",
        "\n",
        "*   Anchor boxes và Model architecture\n",
        "*   Model hyper parameters\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rr1anWZexdEE"
      },
      "source": [
        "### Anchor boxes và Model architecture\n",
        "\n",
        "- Hãy quan sát file yolov3/models/yolov3.yaml. Ta sẽ tạo file yolov3/models/coconut.yaml để tự cấu hình.\n",
        "- Phần architecture gồm backbone và head bạn có thể tự thay đổi ở nhà để thử nghiệm, trong bài thực hành này ta sẽ giữ nguyên architecture của yolov3\n",
        "- Tham số _nc_ là số lượng lớp của bài toán, ta cần thay đổi cho phù hợp (ở cách train 1 ở trên, ta không chỉ định file config yaml này thì model tự hiểu lấy tham số _nc_ trong file data/coconut.yaml)\n",
        "- Tham số anchors: Trong nhiều bài toán phức tạp, việc tìm anchor boxes mới là cần thiết. Bạn có thể tham khảo repo sau: <a href=https://github.com/decanbay/YOLOv3-Calculate-Anchor-Boxes>https://github.com/decanbay/YOLOv3-Calculate-Anchor-Boxes </a>. Hãy thử thay đổi anchor box của mô hình như sau:\n",
        "\n",
        "```\n",
        "P3/8: 77,34, 84,50, 87,70\n",
        "\n",
        "P4/16: 100,43, 107,59, 117,75\n",
        "\n",
        "P5/32: 128,50, 142,66, 153,87\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tztgjl7Lz7Sj"
      },
      "source": [
        "### Model hyper parameter\n",
        "\n",
        "Các siêu tham số cho quá trình huấn luyện mô hình được lưu ở các file yaml trong thư mục yolov3/data/hyps. Hãy quan sát file hyp.scratch.yaml và thay đổi theo ý muốn của bạn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SAkEBi6V0Zm7"
      },
      "source": [
        "### Training model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UAnNcXEmvnCq"
      },
      "outputs": [],
      "source": [
        "!python train.py --img 416 --batch 16 --epochs 20 --data data/coconut.yaml --weights yolov3.pt --cfg models/coconut.yaml --hyp data/hyps/hyp.scratch.yaml"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X4cV_TtM02uL"
      },
      "source": [
        "# 5. Làm chủ pipeline inference model đã train\n",
        "\n",
        "Ở phần trên chúng ta sử dụng file detect.py để infer mô hình đã huấn luyện, ở phần này chúng ta sẽ dựa vào file detect.py, viết 1 hàm detector ngắn gọn, nhận vào 1 ảnh, trả lại tọa độ các bounding box của object. Việc viết hàm này sẽ giúp phần lập trình thuật toán tracking dễ dàng hơn trong các buổi sau"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6IjVeIWm1lmm",
        "outputId": "af84b978-4f6b-414f-db88-855851f424c5"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "YOLOv3 🚀 v9.6.0-3-gb870de5 torch 1.10.0+cu111 CPU\n",
            "\n",
            "Fusing layers... \n",
            "Model Summary: 261 layers, 61497430 parameters, 0 gradients, 154.7 GFLOPs\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "DetectMultiBackend() 32\n",
            "[]\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import sys\n",
        "import cv2\n",
        "import torch\n",
        "import torch.backends.cudnn as cudnn\n",
        "import numpy as np\n",
        "\n",
        "from models.common import DetectMultiBackend\n",
        "from utils.datasets import IMG_FORMATS, VID_FORMATS, LoadImages, LoadStreams\n",
        "from utils.general import (LOGGER, check_file, check_img_size, check_imshow, check_requirements, colorstr,\n",
        "                           increment_path, non_max_suppression, print_args, scale_coords, strip_optimizer, xyxy2xywh)\n",
        "from utils.plots import Annotator, colors, save_one_box\n",
        "from utils.torch_utils import select_device, time_sync\n",
        "from utils.augmentations import letterbox\n",
        "\n",
        "class YOLO():\n",
        "  def __init__(self):\n",
        "    self.img_size = [416, 416]\n",
        "    self.model_path = 'runs/train/exp/weights/best.pt'\n",
        "    self.iou_threshold = 0.5\n",
        "    self.conf_threshold = 0.6\n",
        "    self.device = 'cpu'\n",
        "    self.device = select_device(self.device)\n",
        "\n",
        "    # Initial model\n",
        "    self.model = self.load_models()\n",
        "    self.stride = self.model.stride\n",
        "    self.classes = self.model.names\n",
        "    print(self.model, self.stride)\n",
        "\n",
        "  def load_models(self):\n",
        "    model = DetectMultiBackend(self.model_path, device=self.device)\n",
        "    model.model.float()\n",
        "    # Warmup\n",
        "    model(torch.zeros(1, 3, *self.img_size).to(self.device).type_as(next(model.model.parameters())))\n",
        "    return model\n",
        "\n",
        "  def preprocess_image(self, image):\n",
        "    image = letterbox(image, self.img_size, stride=self.stride)[0]\n",
        "    image = image.transpose((2, 0, 1))[::-1]  # HWC to CHW, BGR to RGB\n",
        "    image = np.ascontiguousarray(image)\n",
        "\n",
        "    return image\n",
        "\n",
        "  def detect(self, image):\n",
        "    im = np.array([self.preprocess_image(image)])\n",
        "    im = torch.from_numpy(im).to(self.device)\n",
        "    im = im.float() / 255.0\n",
        "    pred = self.model(im)\n",
        "    pred = non_max_suppression(pred, self.conf_threshold, self.iou_threshold)\n",
        "\n",
        "    original_image = image.copy()\n",
        "    centers = []\n",
        "    bboxes = []\n",
        "    obj_type = []\n",
        "\n",
        "    for i, det in enumerate(pred):\n",
        "      det[:, :4] = scale_coords(im.shape[2:], det[:, :4], image.shape).round()\n",
        "\n",
        "      for *xyxy, conf, cls in reversed(det):\n",
        "        xyxy = [float(a) for a in xyxy]\n",
        "        bboxes.append(np.array(xyxy))\n",
        "\n",
        "        x_center = (xyxy[0] + xyxy[2]) / 2\n",
        "        y_center = (xyxy[1] + xyxy[3]) / 2\n",
        "        centroid = np.array([[x_center], [y_center]])\n",
        "        centers.append(np.round(centroid))\n",
        "\n",
        "        predicted_class = self.classes[int(cls)]\n",
        "        obj_type.append(predicted_class)\n",
        "\n",
        "    return original_image, centers, bboxes, obj_type\n",
        "\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "  import matplotlib.pyplot as plt\n",
        "\n",
        "  image = cv2.imread('../datasets/images/5.png')\n",
        "  #############\n",
        "# Khởi tạo YOLO\n",
        "#############\n",
        "  original_image, centers, bboxes, obj_type = detector.detect(image)\n",
        "  print(centers)\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}